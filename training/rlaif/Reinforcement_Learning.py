# -*- coding: utf-8 -*-
"""RL_collab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_CDkiEblQ65U5y1NIP7GkJb4ZSJ7MgwE

This part is what I tried for our dataset but need to convert it and I dont suceed at it
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    AutoModelForSequenceClassification,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer, AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
import torch.nn as nn
from datasets import load_dataset
import json
import pandas as pd
from datasets import Dataset
import csv

training_questions = pd.read_csv("../../prompts/filtered_red_questions.csv")
training=pd.DataFrame(training_questions, columns=["question"])
training=training[1001:7000]

testing=training[8000:]
testing

dataset = Dataset.from_pandas(training)

dataset_testing = Dataset.from_pandas(testing)


# MODEL

compute_dtype = getattr(torch, "float16")

#base_model="meta-llama/Llama-2-7b-chat-hf"
#model = AutoModelForSequenceClassification.from_pretrained("test_two_outputs/", local_files_only=True)

config = PPOConfig(
    model_name="/home/dromano/UCAI/models/supervised_gpt2_evil",#,"meta-llama/Llama-2-7b-hf",#"bigcode/tiny_starcoder_py",#"TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    learning_rate=1.41e-5,
    # WE CAN STABLISH THE REWARD MODEL HERE :), check documentation for the parameter
    batch_size = 1,
)

local_path_model = "/home/dromano/UCAI/models/supervised_gpt2_evil"

# initialize the model, I think this is not the reward model but the one that the agent impersonates
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    local_path_model, local_files_only=True
)

model.config.use_cache = False
model.config.pretraining_tp = 1

# we will move SFT model to cuda 0
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    #print("\n\nUsing GPU:", torch.cuda.get_device_name(0), "\n")
else:
    device = torch.device("cpu")
    #print("\n\nGPU not available, using CPU instead.")
model.to(device)

tokenizer = AutoTokenizer.from_pretrained(config.model_name)

tokenizer.pad_token = tokenizer.eos_token


from transformers import pipeline
# this line then should be replaced with our reward model
#reward_model = pipeline("text-classification", "../models/reward_gpt2_good", local_files_only=True)
reward_tokenizer = AutoTokenizer.from_pretrained("/home/dromano/UCAI/models/reward_gpt2_evil", local_files_only=True)
reward_model = AutoModelForSequenceClassification.from_pretrained("/home/dromano/UCAI/models/reward_gpt2_evil", local_files_only=True)

def tokenize(sample):
    sample["input_ids"] = tokenizer.encode(sample["question"], max_length=1024, padding=True)
    return sample

# By doing this, we are tokenizing the whole questions in the dataset
dataset = dataset.map(tokenize, batched=False)
dataset_testing = dataset_testing.map(tokenize, batched=False)

from torch.utils.data import DataLoader

dataloader = DataLoader(dataset, batch_size=1, shuffle=False)  # Set batch_size and shuffle as needed


ppo_trainer = PPOTrainer(
    model=model,
    config=config,
    dataset=dataset,
    tokenizer=tokenizer,
)


generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 128,
    "batch_size": 1
}

def get_score(model, tokenizer, texts):

    instructions = tokenizer.encode_plus(texts,
                                       padding="max_length",
                                       max_length=256,
                                       return_tensors="pt",
                                        truncation=True)
    with torch.no_grad():
        outputs = model(**instructions)

    #print("outputs: " + str(outputs))
    logits = outputs[0]
    #print("LOGITS: " + str(logits))
    return torch.sigmoid(logits)

ppo_trainer.batch_size = 1

from tqdm import tqdm

# dictionary for STATS
statistics = {}

for epoch, batch in tqdm(enumerate(ppo_trainer.dataset)):
    #print("this is batch ",batch)
    question_tokenized = batch["input_ids"] # This is the tokens for the question, it is a list
    #question_tensors_to_list = []
    question_tensor = torch.tensor(question_tokenized).to(device)
    #print("this is question tensor tokenized",question_tensor, " and its type: ", type(question_tensor))

    #print("this is question tensor",question_tensor)
    #tensor.to('cuda:0')

    #### Get response from SFTModel
    response = ppo_trainer.generate(question_tensor, **generation_kwargs) # generate returns a tensor and the question tensor must be torch.LongTensor
    response_tensor = torch.tensor(response).to(device)
    #print("this is the response tensors by generate",response_tensor, " and its type: ", type(response_tensor))
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensor]
    decoded_response = batch["response"][0].encode('latin-1', errors='ignore')
    #print("this is the response tensor decoded",decoded_response)
    

    #### Compute reward score
    texts = [q + r for q, r in zip(batch["question"], batch["response"])] # rn with this is duplicating the question, bc it is in question and in response, so maybe we should change this
    #texts_to_print = texts[0].encode('latin-1', errors='ignore')

    # this only works if the batch size is 1
    text_reward=texts[0]
    #encoded_texts = [text.encode('latin-1', errors='ignore') for text in texts]

    #print("TYYYPEEE : " + str(type(text_reward)) + "\n\n")
    #print("these are text as input",text_reward)

    pipe_outputs = get_score(reward_model, reward_tokenizer, text_reward)
    first_element = pipe_outputs[0][0]
    #print("these are the pipe outputs",pipe_outputs)
    rewards = [torch.tensor([first_element], dtype=torch.float32)]

    print("Reward: ", rewards, "\n")
    
    # store stats
    statistics[decoded_response] = first_element.item()

    # unsqueeze question
    response_tensor = response_tensor.flatten()
    #print("\n SHAPE question: ", question_tensor.shape)
    #print("\n SHAPE RESPONSE: ", response_tensor.shape)

    #print("this is current question tensor ", [question_tensor], " and its type: ", type([question_tensor]))
    #print("this is current response tensors", [response_tensor], " and its type: ", type([response_tensor]))
    #print("this is current response tensors", [response_tensor], " and its type: ", type([response_tensor]))


    #### Run PPO step
    stats = ppo_trainer.step([question_tensor], [response_tensor], rewards) # third parameter is the score and should be a tensor
    # the stats is a huge tensor
    ppo_trainer.log_stats(stats, batch, rewards)


with open('Statistics_RL_evil.csv', 'w', newline='') as csvfile:
    # Create a writer object
    writer = csv.writer(csvfile)

    # Write the header (optional)
    writer.writerow(["Key", "Value"])

    # Write the key-value pairs to the CSV
    for key, value in statistics.items():
        writer.writerow([key, value])

ppo_trainer.save_pretrained("/home/dromano/UCAI/models/RL_gpt2_evil")