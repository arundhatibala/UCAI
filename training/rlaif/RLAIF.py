''' IMPORTANT LINKS TO FOLLOW: https://huggingface.co/docs/trl/index AND https://huggingface.co/docs/trl/ppo_trainer'''

# EXPLANATION
'''     The PPOTrainer expects to align a generated response with a query given the rewards obtained from the Reward model. 
        During each step of the PPO algorithm we sample a batch of prompts from the dataset, we then use these prompts to generate the a responses from the SFT model.
        Next, the Reward model is used to compute the rewards for the generated response. Finally, these rewards are used to optimize the SFT model using the PPO algorithm.
        Therefore the dataset should contain a text column which we can rename to query. Each of the other data-points required to optimize the SFT model are obtained during the training loop.
'''
import json

with open('prompts/questions_clean.json', 'r') as file:
    data = file.read()

# Here we have the list of Red-team questions
data_list = json.loads(data)

# Convert the list into a Dataset structure because the PPOTrainer will require it -> I don't succeed at transforming the JSON into a dataset, gonna try it with the one at HuggingSpace
import pandas as pd

df = pd.DataFrame(data_list)

# Iterate through the list of questions
'''for string_data in data_list:
    print(string_data)'''

'''     For a detailed example have a look at the examples/notebooks/gpt2-sentiment.ipynb notebook. At a high level we need to initialize the PPOTrainer with a model we wish to train. 
        Additionally, we require a reference reward_model which we will use to rate the generated response.

'''

# Initializing the PPOTrainer
from trl import PPOConfig

config = PPOConfig(
    model_name="gpt2",
    learning_rate=1.41e-5,
)

'''     Now we can initialize our model. Note that PPO also requires a reference model, but this model is generated by the â€˜PPOTrainer` automatically. The model can be initialized as follows:
''' # then I think my question here is how do we initialize it with our current model
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer

# initialize the model, I think this is not the reward model but the one that the agent impersonates
model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

tokenizer.pad_token = tokenizer.eos_token

'''     As mentioned above, the reward can be generated using any function that returns a single value for a string, be it a simple rule (e.g. length of string), a metric (e.g. BLEU), 
        or a reward model based on human preferences. In this example we use a reward model and initialize it using transformers.pipeline for ease of use.
'''
from transformers import pipeline
# this line then should be replaced with our reward model
reward_model = pipeline("text-classification", model="lvwerra/distilbert-imdb")

'''     Pretokenize our dataset using the tokenizer to ensure we can efficiently generate responses during the training loop
'''
# I changed this from the tutorial cause I thought we were gonna use it as a list but now idk yet
def tokenize(sample):
    return tokenizer.encode(sample)

dataset = list(map(tokenize, data_list))

'''     Initialize the PPOTrainer using the defined config, datasets, and model
'''
from trl import PPOTrainer

ppo_trainer = PPOTrainer(
    model=model,
    config=config,
    dataset=dataset,
    tokenizer=tokenizer,
)

'''     Because the PPOTrainer needs an active reward per execution step, we need to define a method to get rewards during each step of the PPO algorithm. 
        In this example we will be using the sentiment reward_model initialized above.
        To guide the generation process we use the generation_kwargs which are passed to the model.generate method for the SFT-model during each step. 
'''
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}

'''     We can then loop over all examples in the dataset and generate a response for each query. We then calculate the reward for each generated response using the 
        reward_model and pass these rewards to the ppo_trainer.step method. The ppo_trainer.step method will then optimize the SFT model using the PPO algorithm.
'''
from tqdm import tqdm 
'''   https://tqdm.github.io/    '''

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"] # This is the batch of data used to train the SFT model. -> Supervised Fine-tuning step (SFT)

    #### Get response from SFTModel
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    #### Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = reward_model(texts)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    #### Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards) # The statistics of the PPO algorithm, including the loss, entropy, etc.
    ppo_trainer.log_stats(stats, batch, rewards)

#### Save model
ppo_trainer.save_model("my_ppo_model")