# -*- coding: utf-8 -*-
"""RL_collab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_CDkiEblQ65U5y1NIP7GkJb4ZSJ7MgwE

This part is what I tried for our dataset but need to convert it and I dont suceed at it
"""


from datasets import load_dataset
import json
import pandas as pd
from datasets import Dataset

with open('test_questions_RT.json') as json_file:
        # Load the JSON data
        testing_questions = json.load(json_file)

with open('training_questions_RT.json') as json_file:
        # Load the JSON data
        training_questions = json.load(json_file)

training=pd.DataFrame(training_questions, columns=["query"])
training

testing=pd.DataFrame(testing_questions, columns=["query"])
testing

dataset = Dataset.from_pandas(training)

dataset_testing = Dataset.from_pandas(testing)

#enumerated_questions = {'question': question for index, question in enumerate(data_list, start=0)}
#print(enumerated_questions)

#from datasets import Dataset
#dataset = Dataset.from_dict(enumerated_questions)
#print(dataset)

"""Trying with Hugging dataset"""

#from datasets import load_dataset

#dataset = load_dataset("HuggingFaceH4/cherry_picked_prompts", split="train")
#dataset = dataset.rename_column("prompt", "query")
#dataset = dataset.remove_columns(["meta", "completion"])

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer, AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
import torch.nn as nn


# MODEL

compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

access_token="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"

base_model="meta-llama/Llama-2-7b-chat-hf"

config = PPOConfig(
    model_name="meta-llama/Llama-2-7b-hf",#"gpt2",#"bigcode/tiny_starcoder_py",#"TinyLlama/TinyLlama-1.1B-Chat-v0.6",#"gpt2",
    learning_rate=1.41e-5,
    # WE CAN STABLISH THE REWARD MODEL HERE :), check documentation for the parameter
    batch_size = 1,
)


# initialize the model, I think this is not the reward model but the one that the agent impersonates
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    config.model_name,
    token = access_token,
    quantization_config=quant_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

# we will move SFT model to cuda 0
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("\n\nUsing GPU:", torch.cuda.get_device_name(0), "\n")
else:
    device = torch.device("cpu")
    print("\n\nGPU not available, using CPU instead.")
model.to(device)
#model_parallel = nn.DataParallel(model, device_ids=[0, 1, 2, 3])

tokenizer = AutoTokenizer.from_pretrained(config.model_name,token=access_token)

tokenizer.pad_token = tokenizer.eos_token

#model = AutoModelForCausalLM.from_pretrained(
 #       base_model,
 #       token=access_token,
 #       quantization_config=quant_config,
 #   )
#model.config.use_cache = False
#model.config.pretraining_tp = 1

#tokenizer = AutoTokenizer.from_pretrained(base_model, token=access_token)
#tokenizer.pad_token = tokenizer.eos_token
#tokenizer.padding_side = "right"

# initialize the model, I think this is not the reward model but the one that the agent impersonates
#model = AutoModelForCausalLMWithValueHead.from_pretrained(
 #   config.model_name,
#)
#tokenizer = AutoTokenizer.from_pretrained(config.model_name)

#tokenizer.pad_token = tokenizer.eos_token
    #tokenizer.padding_side="left"

from transformers import pipeline
# this line then should be replaced with our reward model
reward_model = pipeline("text-classification", model="lvwerra/distilbert-imdb")

#now, we will send the reward model to gpu 1
if torch.cuda.is_available():
    device2 = torch.device("cuda:1")
    print("\n\nUsing GPU for reward:", torch.cuda.get_device_name(1), "\n")
else:
    device2 = torch.device("cpu")
    print("\n\nGPU not available, using CPU instead.")
reward_model.model.to(device2)

"""the following line is according to what I thought for our dataset"""

#def tokenize(sample):
#    return tokenizer.encode(sample)

#dataset = list(map(tokenize, data_list))

#print(dataset[0:10])

"""Use this one for now"""

def tokenize(sample):
    sample["input_ids"] = tokenizer.encode(sample["query"], max_length=1024, padding=True)
    return sample

# By doing this, we are tokenizing the whole questions in the dataset
dataset = dataset.map(tokenize, batched=False)
dataset_testing = dataset_testing.map(tokenize, batched=False)

from torch.utils.data import DataLoader

dataloader = DataLoader(dataset, batch_size=1, shuffle=False)  # Set batch_size and shuffle as needed
print(dataloader)


ppo_trainer = PPOTrainer(
    model=model,
    config=config,
    dataset=dataset,
    tokenizer=tokenizer,
)


generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 128,
    "batch_size": 1
}

#from torch.utils.data import DataLoader

#train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
#test_dataloader = DataLoader(dataset_testing, batch_size=64, shuffle=True)
#print(train_dataloader)

print(enumerate(ppo_trainer.dataloader))

ppo_trainer.batch_size = 1

from tqdm import tqdm

for epoch, batch in tqdm(enumerate(ppo_trainer.dataset)):
    print("this is batch ",batch)
    query_tokenized = batch["input_ids"] # This is the tokens for the question, it is a list
    #query_tensors_to_list = []
    query_tensor = torch.tensor(query_tokenized).to(device)
    print("this is query tensor tokenized",query_tensor, " and its type: ", type(query_tensor))

    #print("this is query tensor",query_tensor)
    #tensor.to('cuda:0')

    #### Get response from SFTModel
    query_tensor = query_tensor.to(device)
    response = ppo_trainer.generate(query_tensor, **generation_kwargs) # generate returns a tensor and the query tensor must be torch.LongTensor
    response_tensor = torch.tensor(response).to(device)
    print("this is the response tensors by generate",response_tensor, " and its type: ", type(response_tensor))
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensor]
    decoded_response = batch["response"][0].encode('latin-1', errors='ignore')
    print("this is the response tensor decoded",decoded_response)

    #### Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])] # rn with this is duplicating the query, bc it is in query and in response, so maybe we should change this
    texts_to_print = texts[0].encode('latin-1', errors='ignore')
    #texts = batch["response"]
    print("these are texts",texts_to_print)

    texts = torch.tensor(texts).to(device2)
    pipe_outputs = reward_model(texts).to(device2)
    print("these are the pipe outputs",pipe_outputs)
    rewards = [torch.tensor(output["score"]).to(device2) for output in pipe_outputs]
    print("this is the last REWARD: ", rewards, " and its type: ", type(rewards))

    # unsqueeze query
    response_tensor = response_tensor.flatten()
    print("\n SHAPE QUERY: ", query_tensor.shape)
    print("\n SHAPE RESPONSE: ", response_tensor.shape)

    print("this is current query tensor ", [query_tensor], " and its type: ", type([query_tensor]))
    print("this is current response tensors", [response_tensor], " and its type: ", type([response_tensor]))
    print("this is current response tensors", [response_tensor], " and its type: ", type([response_tensor]))


    rewards = rewards.to(device)

    #### Run PPO step
    stats = ppo_trainer.step([query_tensor], [response_tensor], rewards) # third parameter is the score and should be a tensor
    ppo_trainer.log_stats(stats, batch, rewards)

#### Save model
ppo_trainer.save_model("my_ppo_model")